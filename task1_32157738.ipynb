{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Task 1, Assessment 1 : FIT 5196 </h1>\n",
    "\n",
    "**Student Name : Potluri Karthik Venkat Chowdary**\n",
    "\n",
    "**Student ID : 32157738**\n",
    "\n",
    "**Date : 13/04/2021**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Introduction </h2>\n",
    "\n",
    "In this task we will be extracting text from semi-structured, this is the first step of analyzing any textual data. In real world textual data is always obtained in formats that are hard to extract from and perform any operations on. We can use various techniques to extract the requried information from them. In this project we will be mainly using regular expressions. \n",
    "\n",
    "<h4> a. Data Description </h4>\n",
    "\n",
    "- We have a collection of text files each contains information about several news articles including the article itself.\n",
    "- News articles contain information from across the globe and might also include articles that are not of English language.\n",
    "\n",
    "**Input Files :**\n",
    "\n",
    "- We have around 15 text files each containing information about several news articles.\n",
    "- Our object of interest among the several other data points would be the uuid, author, Published and text.\n",
    "\n",
    "**Possible Roadblocks :**\n",
    "\n",
    "- Extracting the exact information that is neccesary.\n",
    "- Excluding Articles that are not in English.\n",
    "- Converting special symbols.\n",
    "- Avoiding commas while writing to CSV\n",
    "\n",
    "\n",
    "<h4> b. Libraries </h4>\n",
    "\n",
    "For this project we will be keeping our use of libraries to minimum. We will be using only three libraries, which are as follows: \n",
    "\n",
    "- **re :** We will be using re for operating with regular expressions.\n",
    "- **os :** os will be used to read and write information and manipulate files.\n",
    "- **langid :** We will be using Langid to classify the language of articles to exclude articles that are not in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Packages\n",
    "import re\n",
    "import os\n",
    "import langid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Parsing files and Extracting information </h2>\n",
    "\n",
    "<h4> a. Parsing files </h4>\n",
    "\n",
    "We will be using the package OS while parsing the files and reading in the information. We will be going through all the files in the folder \"32157738\" and saving their names into a list called files_shake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files_shake = []\n",
    "for root, dirs, files in os.walk(\"./32157738/\"):\n",
    "    for filename in files:\n",
    "        files_shake.append(\"./32157738/\"+filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> b. Extracting information </h4>\n",
    "\n",
    "Two extract the data using regular expression it is very important to understand the form in which data is currently in the text file.Each text file have around a 100 news articles in them along with a lot of details about these news articles. Regular expression searches patterns and strings to return the required pattern or string. However a 8-bit string and an unicode cannot be matched using regular expressions (Re — Regular Expression Operations — Python 3.9.4 Documentation, n.d.). Let us understand the existing data better, before proceding to create our regular expressions.\n",
    "\n",
    "If we look at the text files we can observe that they look something like this\n",
    "\n",
    "$$   \n",
    ".... \"uuid\": \\textit{\"uuid\"}, \\hspace{3mm} \"author\": \\textit{\"author\"}, \\hspace{3mm} \"published\": \\textit{\"published\"}, \\hspace{3mm} \"text\": \\textit{\"text\"}, \\hspace{3mm} \"country\": .... \n",
    "$$\n",
    "\n",
    "Let us first concentrate on getting uuid from this, \n",
    "- To obtain uuid we know that we have to search for text after ___\"uuid\": \"___ \n",
    "- And uuid ends before author starts, so uuid is the value that is between ___\"uuid\": \"___ and ___\" \"author\"___\n",
    "- There is a problem here, as there are multiple ___\"uuid\": \"___ and ___\" \"author\"___ we need to specify in the regular expression to extract the information between to succesive ___\"uuid\": \"___ and ___\" \"author\"___ . We can do this by using a ? symbol. \n",
    "- while representing symbols such as ' _\"_ ', ' _:_ ' which are also present in regular expressions we need to put a back slash before using them.\n",
    "- Another important step while using the regular expression is representing the white space, to represent the white space we will be using a _'/s'_ .\n",
    "- We will be going through each text file in _files_shake_ and use ___re.findall___ to get every possible occurance that satisfies our conditions.\n",
    "\n",
    "Similarly by using the above process we can extract other attributes (author, published and text) from the text file.\n",
    "\n",
    "- I performed a small check here to compare the number of uuid, author, published and text. Since they all should be equal. \n",
    "- If they are all equal then we cann add them to the list of uuid (_uuid_list_), author (_author_list_), published (_published_list_), and (_text_list_).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "uuid_list = []\n",
    "author_list = []\n",
    "published_list = []\n",
    "text_list = []\n",
    "for file_name in files_shake:\n",
    "    f = open(file_name, \"r\", encoding = \"utf8\")\n",
    "    temp_file = f.read()\n",
    "    uuid_list_temp = re.findall('\\\"uuid\\\"\\:\\s\\\"(.*?)\\\"\\,\\s\\\"author\\\"',temp_file,flags=re.DOTALL)\n",
    "    author_list_temp = re.findall('\\\"author\\\"\\:\\s\\\"(.*?)\\\"\\,\\s\\\"published\\\"',temp_file,flags=re.DOTALL)\n",
    "    published_list_temp = re.findall('\\\"published\\\"\\:\\s\\\"(.*?)\\\"\\,\\s\\\"text\\\"',temp_file,flags=re.DOTALL)\n",
    "    text_list_temp = re.findall('\\\"text\\\"\\:\\s\\\"(.*?)\\\"\\,\\s\\\"country\\\"',temp_file,flags=re.DOTALL)\n",
    "    \n",
    "    if(len(uuid_list_temp)==len(author_list_temp)==len(published_list_temp)==len(text_list_temp)):\n",
    "        uuid_list.extend(uuid_list_temp)\n",
    "        author_list.extend(author_list_temp)\n",
    "        published_list.extend(published_list_temp)\n",
    "        text_list.extend(text_list_temp)\n",
    "    else:\n",
    "        print(\"something is wrong in the file \"+file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> c. Including English only articles </h4>\n",
    "\n",
    "- Among the articles we have now, some of the articles are not in English, we are not interested in these articles. We will be removing them.\n",
    "- To check whether an article is in English or not we will be using classify function of langid. This gives a result like this : ('en' , 349030130 ) if the language is english. If it is not english 'en' would be different 'ru' for russian, 'hi' for hindi etc.,\n",
    "- We will be checking each article if it is english are not and if it is english we will be adding it and corresponding uuid, author and published to their own lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "uuid_list_2 = []\n",
    "author_list_2 = []\n",
    "published_list_2 = []\n",
    "text_list_2 = []\n",
    "list_length = len(uuid_list)\n",
    "for i in range(list_length):\n",
    "    if langid.classify(text_list[i])[0] == 'en':\n",
    "        uuid_list_2.append(uuid_list[i])\n",
    "        text_list_2.append(text_list[i].replace('\\\\n',''))\n",
    "        author_list_2.append(author_list[i])\n",
    "        published_list_2.append(published_list[i])\n",
    "    else:\n",
    "        continue\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have removed all the articles that are not in English. Our next step would be to convert special characters. \n",
    "\n",
    "<h4> d. Special characters </h4>\n",
    "\n",
    "- If we look at the data now we can observe that there are certain characters which or still in their unicode (\\\\u1234) format.\n",
    "- Unicode is an IT standard used for encoding text expressed in most of the written langauges _(Wikipedia contributors, n.d.)_.\n",
    "- These unicode can be converted into their original symbols by treating them as surrogate pairs _(Surrogate Characters | IOS Internationalization: Characters and Encoding | InformIT, 2015)_.\n",
    "- We will be encoding and decoding the files using the surrogate pass. we will be doing this process until all the articles are devoid of unicode symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop = 1\n",
    "while stop == 0 :\n",
    "    list_length_2 = len(uuid_list_2)\n",
    "    Stop = 0\n",
    "    for j in range(list_length_2):\n",
    "        if re.search(r'(\\\\u){1}',text_list_2[j]) is not None:\n",
    "            text_list_2[j] = \"\\\"\"+text_list_2[j]+\"\\\"\"\n",
    "            text_list_2[j] = eval(text_list_2[j]).encode('utf-16', 'surrogatepass').decode('utf-16')\n",
    "            if re.search(r'(\\\\u){1}',text_list_2[j]) is not None:\n",
    "                stop = 1\n",
    "        else:\n",
    "            text_list_2[j] = text_list_2[j].encode('utf-16', 'surrogatepass').decode('utf-16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Writing to XML and CSV </h2>\n",
    "\n",
    "Now that we have all the english articles in place with no special characters or symbols. We can write them to XML and CSV with small necessary changes.\n",
    "\n",
    "<h4> a. Writing to XML </h4>\n",
    "\n",
    "XML has some special characters in it that cannot exist in the text, If they do it will obstruct the XML format.\n",
    "\n",
    "- To avoid these we will be replacing them with appropriate symbols from XML. \n",
    "- We wiill be writing the existing items from each of the lists one by one into the XML file as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('32157738.xml','w',encoding='utf-8') as f:\n",
    "    f.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>')\n",
    "    f.write('<data>\\n')\n",
    "    for i in range(len(uuid_list_2)):\n",
    "        f.write('<item>\\n')\n",
    "        f.write('<uuid>'+uuid_list_2[i].replace(\"\\\\n\", \" \").replace(\"\\\\\", \"&#92;\").replace(\"<\", \"&#lt;\").replace(\">\", \"&#62;\").replace(\"\\'\", \"&apos;\").replace(\"&\", \"&amp;\").replace(\"\\\"\", \"&quot;\")+'</uuid>\\n')\n",
    "        f.write('<author>'+author_list_2[i].replace(\"\\\\n\", \" \").replace(\"\\\\\", \"&#92;\").replace(\"<\", \"&#lt;\").replace(\">\", \"&#62;\").replace(\"\\'\", \"&apos;\").replace(\"&\", \"&amp;\").replace(\"\\\"\", \"&quot;\")+'</author>\\n')\n",
    "        f.write('<published>'+published_list_2[i].replace(\"\\\\n\", \" \").replace(\"\\\\\", \"&#92;\").replace(\"<\", \"&#lt;\").replace(\">\", \"&#62;\").replace(\"\\'\", \"&apos;\").replace(\"&\", \"&amp;\").replace(\"\\\"\", \"&quot;\")+'</published>\\n')\n",
    "        f.write('<text>'+text_list_2[i].replace(\"\\\\n\", \" \").replace(\"\\\\\", \"&#92;\").replace(\"<\", \"&#lt;\").replace(\">\", \"&#62;\").replace(\"\\'\", \"&apos;\").replace(\"&\", \"&amp;\").replace(\"\\\"\", \"&quot;\")+'</text>\\n')\n",
    "        f.write('</item>\\n')\n",
    "    f.write('</data>\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>b. Writing to CSV </h4>\n",
    "\n",
    "While writing to CSV the characters that would prove as issue would be  Comma(,), Double quotes (\"), Line break(\"\\n\") so as a blanket measure we will be replacing them with multiple quotes so we would not stumble into problems with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('32157738.csv','w',encoding='utf-8') as h:\n",
    "    h.write(\"uuid,author,published,text\\n\")\n",
    "    for i in range(len(uuid_list_2)):\n",
    "        h.write('\"'+uuid_list_2[i].replace(\"\\n\",\"\").replace('\"', '\"\"')+'\"'+\",\"+'\"'+author_list_2[i].replace(\"\\n\",\" \").replace('\"', '\"\"')+'\"'+\",\"+'\"'+published_list_2[i].replace(\"\\n\",\"\").replace('\"', '\"\"')+'\"'+\",\"+'\"'+text_list_2[i].replace(\"\\'\",\"'\").replace(\"\\n\",\"\").replace('\"', '\"\"')+'\"'+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> References </h2>\n",
    "\n",
    "- re — Regular expression operations — Python 3.9.4 documentation. (n.d.). Python.Org. https://docs.python.org/3/library/re.html\n",
    "- Wikipedia contributors. (n.d.). Unicode. Wikipedia. https://en.wikipedia.org/wiki/Unicode\n",
    "- Surrogate Characters | iOS Internationalization: Characters and Encoding | InformIT. (2015, January 21). Informit. https://www.informit.com/articles/article.aspx?p=2274038&seqNum=10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
